{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AWGzucuFfbBn"
   },
   "source": [
    "\n",
    "# Basic Prompting with LangChain\n",
    "\n",
    "LangChain is a popular framework that allow users to quickly build apps and pipelines around **L**arge **L**anguage **M**odels. It can be used to for chatbots, **G**enerative **Q**uestion-**A**nwering (GQA), summarization, and much more.\n",
    "\n",
    "\n",
    "* **Prompt templates**: Prompt templates are templates for different types of prompts\n",
    "\n",
    "* **LLMs**: Large language models used here FLAT-T5 -large\n",
    "\n",
    "\n",
    "**Objectives** : We use a Large language model to understand different styles of prompting using **Langchain Prompt Templates**\n",
    "1. Logical Thinking\n",
    "2. Chain of Thought Prompting\n",
    "3. Prompt Chaining\n",
    "\n",
    "**Note** : Langchain is a comprehensive tool chain for promoting. This is a basic examples demonstrate chaining concepts. In future I plan to write more on some advanced Prompt chaining with Langchain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r-ryCeG_f_GC",
    "outputId": "150d4ff6-25a9-4dd9-c670-af8c5a55a346",
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -qU langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mNaXrEPOhbuL"
   },
   "source": [
    "## Setting up Hugging Face\n",
    "\n",
    "- We need to make sure to have a Hugging Face Account and a Huggingface API token. To do this, login to HF and create an Account first.\n",
    "- For Hugging Face models we need a Hugging Face Hub API token. We can find this by first getting an account at HuggingFace.co and clicking on our profile in the top-right corner > click Settings > click Access Tokens > click New Token > set Role to write > Generate > copy and paste the token below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "sRGTytxCjKaW",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "#set the Huggingface API tokens to work with Lang chain\n",
    "os.environ['HUGGINGFACEHUB_API_TOKEN'] = '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' # replace this with your own token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "exAl3iQgnAra"
   },
   "source": [
    "We can then generate text using a HF Hub model (we'll use `google/flan-t5-large`) using Langchain's prompt Template format below\n",
    "\n",
    "_(The default Inference API doesn't use specialized hardware and so can be slow and cannot run larger models like `google/flan-t5-xxl`)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 353
    },
    "id": "l7yubiSJhIfs",
    "outputId": "071f7cf4-7307-42dd-9bbe-5c72714a559e",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#from langchain import PromptTemplate, HuggingFaceHub, LLMChain\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "from langchain.llms import HuggingFaceHub\n",
    "\n",
    "\n",
    "# initialize HF LLM\n",
    "model = HuggingFaceHub(\n",
    "    repo_id=\"google/flan-t5-large\",\n",
    "    model_kwargs={\"temperature\":1e-50}\n",
    ")\n",
    "\n",
    "# build prompt template for simple question-answering\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: \"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "\n",
    "llm_chain = LLMChain(\n",
    "    prompt=prompt,\n",
    "    llm=model\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am not moving because I am not moving.\n"
     ]
    }
   ],
   "source": [
    "# Prompting type 1 : logical thinking\n",
    "question = \"\"\"\n",
    "I am riding a bicycle. The pedals are moving fast. I look into the\n",
    "mirror and I am not moving. Why is this?\n",
    "\"\"\"\n",
    "print(llm_chain.run(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "They had 10 - 4 = 6 bananas left. Therefore, the final answer is 6.\n"
     ]
    }
   ],
   "source": [
    "# Prompting type 2:  Chain of Thought Prompting \n",
    "question=\"Answer the following by reasoning step-by-step: A Cafeteria had 10 bananas. If they used 4 for lunch. how many bananas do they have ?\"\n",
    "print(llm_chain.run(question))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DXroZSjCKxa2"
   },
   "source": [
    "If we'd like to ask multiple questions we can by passing a list of dictionary objects, where the dictionaries must contain the input variable set in our prompt template (`\"question\"`) that is mapped to the question we'd like to ask."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7zoxlXHYLQix"
   },
   "source": [
    "**Grouping Questions Together** : It is a LLM, so we can try feeding in all questions at once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b96WIvouLQ-7",
    "outputId": "c9ff1c1f-2991-4832-d57c-b46cc346ca64",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Berlin is the capital of Germany. The first person to walk on the moon was Buzz Aldrin\n"
     ]
    }
   ],
   "source": [
    "multi_template = \"\"\"Answer the following questions one at a time.\n",
    "\n",
    "Questions:\n",
    "{questions}\n",
    "\n",
    "Answers:\n",
    "\"\"\"\n",
    "long_prompt = PromptTemplate(\n",
    "    template=multi_template,\n",
    "    input_variables=[\"questions\"]\n",
    ")\n",
    "\n",
    "llm_chain = LLMChain(\n",
    "    prompt=long_prompt,\n",
    "    llm=model\n",
    ")\n",
    "\n",
    "qs_str = (\n",
    "    \"What is the capital of Germany?\" +\n",
    "    \"Who was the 1st person on the moon?\" \n",
    ")\n",
    "\n",
    "print(llm_chain.run(qs_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# let us make sure we release any memory and invoke Garbage Collector before we move to other notebooks\n",
    "# free up the memory\n",
    "import gc\n",
    "import torch\n",
    "# del model\n",
    "# del llm_chain\n",
    "# del long_prompt\n",
    "# del prompt\n",
    "\n",
    "\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_memory: 14.76 GB\n",
      "Available GPU memory: 14.76 GB\n"
     ]
    }
   ],
   "source": [
    "# Let us monitor memory\n",
    "import torch\n",
    "\n",
    "# Retrieve GPU memory statistics\n",
    "memory_stats = torch.cuda.memory_stats()\n",
    "# Retrieve maximum GPU memory allocated by PyTorch\n",
    "max_memory_allocated = torch.cuda.max_memory_allocated()\n",
    "# Calculate available GPU memory\n",
    "total_memory = torch.cuda.get_device_properties(0).total_memory\n",
    "available_memory = total_memory - memory_stats[\"allocated_bytes.all.current\"]\n",
    "\n",
    "# Print the result\n",
    "print(f\"total_memory: {total_memory / 1024**3:.2f} GB\")\n",
    "# print(f\"Peak GPU memory allocated by PyTorch: {max_memory_allocated / 1024**3:.2f} GB\")\n",
    "print(f\"Available GPU memory: {available_memory / 1024**3:.2f} GB\")\n",
    "\n",
    "\n",
    "## Make sure you are able to Total Memory of 14GB before moving to the next assisgnment, else restart the session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "b8e7999f96e1b425e2d542f21b571f5a4be3e97158b0b46ea1b2500df63956ce"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
